{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN networks for classification of three different porkchop situations\n",
    "\n",
    "The following code creates three convolutional neural networks:\n",
    "1. A simple CNN that is trained from scratch\n",
    "2. A pre-trained CNN where the convolutional layers from VGG-16 is loaded and used. \n",
    "3. A CNN with the architecture of that of 2. but without any pretrained and locked parameters\n",
    "\n",
    "For simplicity and an easier way to grid-search for optimal hyperparameters, the Keras library is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers, callbacks\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Activation, Dropout, Dense, Input, Conv2D, MaxPooling2D, Flatten, ZeroPadding2D\n",
    "from keras import callbacks\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to make model-making more flexible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To simplify the creation of layers three functions are made\n",
    "## Per standard the convolutional layers have 3x3 kernels, 0.5 dropout, and relu as activation function\n",
    "\n",
    "def add_conv_layer(model, n_filters=32, kernelsize=(3,3), resize_dim=(224,224,3), num_layer=1, dropout=0.5):\n",
    "    if (num_layer==0):\n",
    "        model.add(Conv2D(n_filters, kernelsize, input_shape=resize_dim))\n",
    "    else:\n",
    "        model.add(Conv2D(n_filters, kernelsize))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def add_dense_layer(model, n_neurons, dropout=0.5, activation='relu'):\n",
    "    model.add(Dense(n_neurons))\n",
    "    model.add(Activation(activation))\n",
    "    if dropout:\n",
    "        model.add(Dropout(dropout))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def VGG_add_conv_layer(model, n_filters, reps, trainable=0):    \n",
    "    for i in range(reps):\n",
    "        model.add(ZeroPadding2D((1,1)))\n",
    "        \n",
    "        # Setting trainable = 0 freezes the conv.layers loaded from VGG16\n",
    "        model.add(Conv2D(n_filters, (3, 3), activation='relu', trainable=trainable))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the datagenerators to feed into the networks\n",
    "\n",
    "This code is loading images from the data folders. The point of using these is that they are a simple way to preprocess the images slightly, and hereby make the CNNs less sensitive to overfitting.\n",
    "\n",
    "The following code uses an inbuilt keras functionality called ImageDataGenerator.\n",
    "\n",
    "The code is based on the following link, where further information can be found:\n",
    "\n",
    "https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen(resize_dim, batch_size):\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            shear_range=0.3,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True,\n",
    "            rotation_range=30,\n",
    "            channel_shift_range=15.0\n",
    "            )\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(\n",
    "                    rescale=1./255,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True,\n",
    "                    rotation_range=90)\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "            'data/train',  \n",
    "            target_size=resize_dim[:2],  \n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical') \n",
    "\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "            'data/validation',\n",
    "            target_size=resize_dim[:2],\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical')\n",
    "    \n",
    "    return train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To plot the training pyplot is used\n",
    "\n",
    "For making pretty graphs showing how the models train over time, a function is made to carry this out.\n",
    "\n",
    "The following cell is purely aesthetics - and thus considered non-essential for understanding of the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def model_graph(models, names, file_prefix):\n",
    "    colors_train = ['g--', 'b--', 'r--']\n",
    "    colors_test = ['g', 'b', 'r']\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        plt.figure()\n",
    "        plt.plot(models[i].fit.history['acc'], colors_train[i])\n",
    "        plt.plot(models[i].fit.history['val_acc'], colors_test[i])\n",
    "        plt.ylim(0,1)\n",
    "        plt.yticks(np.arange(0,1.1,0.1))\n",
    "        plt.legend([names[i] + ' train', names[i] + ' test'], loc=4)\n",
    "        plt.title(names[i] + \" performance\", fontname=\"Times New Roman\", fontsize=20)\n",
    "\n",
    "        plt.savefig(\"graphs/\"+ file_prefix + \"_\" + names[i] + \"_acc.png\", dpi=200, facecolor='w', edgecolor='w',\n",
    "                orientation='portrait', papertype=None, format=None,\n",
    "                transparent=False, bbox_inches='tight', pad_inches=0.1,\n",
    "                frameon=None)\n",
    "        plt.close()\n",
    "        \n",
    "    plt.figure()\n",
    "    namelist = []\n",
    "    for i in range(len(models)):\n",
    "        plt.plot(models[i].fit.history['acc'], colors_train[i])\n",
    "        plt.plot(models[i].fit.history['val_acc'], colors_test[i])\n",
    "        namelist.append(names[i] + ' train')\n",
    "        namelist.append(names[i] + ' test')\n",
    "    \n",
    "    plt.ylim(0,1)\n",
    "    plt.legend(namelist, loc=4)\n",
    "    plt.title(\"Model Performances\", fontname=\"Times New Roman\", fontsize=20)\n",
    "\n",
    "    plt.savefig(\"graphs/\"+ file_prefix + \"_\" + names[i] + \"_all_acc.png\", dpi=200, facecolor='w', edgecolor='w',\n",
    "            orientation='portrait', papertype=None, format=None,\n",
    "            transparent=False, bbox_inches='tight', pad_inches=0.1,\n",
    "            frameon=None)\n",
    "    plt.close()\n",
    "    \n",
    "def show_graph(models, names):\n",
    "    plt.figure()\n",
    "    colors_train = ['g--', 'b--', 'r--']\n",
    "    colors_test = ['g', 'b', 'r']\n",
    "    \n",
    "    namelist = []\n",
    "    for i in range(len(models)):\n",
    "        plt.plot(models[i].fit.history['acc'], colors_train[i])\n",
    "        plt.plot(models[i].fit.history['val_acc'], colors_test[i])\n",
    "        namelist.append(names[i] + ' train')\n",
    "        namelist.append(names[i] + ' test')\n",
    "    \n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.arange(0,1.1,0.1))\n",
    "    plt.legend(namelist, loc=4)\n",
    "    plt.title(\"Model Performances\", fontname=\"Times New Roman\", fontsize=20)\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes to make model-tweaking easier\n",
    "\n",
    "The following classes are defined to simplify the creation of networks further down in the code.\n",
    "\n",
    "By making these it is simpler to understand and easier to crossvalidate models.\n",
    "\n",
    "There is one for a normal CNN - used for a baseline model, and one for the VGG-16 architecture models (pretrained and untrained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_model:\n",
    "    \n",
    "    def __init__(self, conv_layers=[128,64,128], dense_layers=[64,16], learning_rate = 0.001, batch_size = 32):\n",
    "        self.model_score = 0\n",
    "        self.conv_layers = conv_layers\n",
    "        self.dense_layers = dense_layers\n",
    "        self.lr = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.resize_dim = (224,224,3)\n",
    "        self.fit = 0\n",
    "        \n",
    "        # Datagen init\n",
    "        self.train_generator, self.validation_generator = datagen(self.resize_dim, self.batch_size)\n",
    "\n",
    "        # Model init\n",
    "        self.model = Sequential()\n",
    "    \n",
    "        # Conv layers\n",
    "        self.model.add(ZeroPadding2D((1,1),input_shape=self.resize_dim[:2]+(3,)))\n",
    "        for l in range(len(conv_layers)):\n",
    "            if (conv_layers[l]):\n",
    "                self.model = add_conv_layer(self.model, conv_layers[l], num_layer = l)\n",
    "    \n",
    "        # Flatten matrix\n",
    "        self.model.add(Flatten())  \n",
    "    \n",
    "        # Dense layers\n",
    "        for l in range(len(dense_layers)):\n",
    "            if (dense_layers[l]):\n",
    "                self.model = add_dense_layer(self.model, dense_layers[l], dropout=0.5)\n",
    "\n",
    "        # Adding the last layer with three options (due to three classes)\n",
    "        self.model.add(Dense(3)) ### Fra 1 til 3\n",
    "        self.model.add(Activation('sigmoid'))\n",
    "    \n",
    "        # Defining the optimizer used for backprobagation\n",
    "        adam_op = optimizers.Adam(lr=self.lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "        \n",
    "        # Compiling the model\n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=adam_op,\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        print(\"Model was created succesfully\\n\")  \n",
    "    \n",
    "    def train_model(self, batch_size = 16, n_epochs=50, score_batch=16, verbose=2):\n",
    "        self.fit = self.model.fit_generator(\n",
    "                    self.train_generator,\n",
    "                    steps_per_epoch=1000 // batch_size,\n",
    "                    epochs=n_epochs,\n",
    "                    validation_data=self.validation_generator,\n",
    "                    validation_steps=400 // batch_size,\n",
    "                    verbose = verbose)\n",
    "        print(\"Training Done\")\n",
    "        \n",
    "    def validate_model(self, score_batch=64):\n",
    "        self.model_score = self.model.evaluate_generator(self.validation_generator, score_batch)\n",
    "        print(\"Validation Done\")\n",
    "        return self.model_score[1]\n",
    "\n",
    "class CNN_model_VGG16:\n",
    "    \n",
    "    def __init__(self, weights_path='vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', dense_layers=[16,8], learning_rate = 0.001, batch_size = 32):\n",
    "        self.lr = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.resize_dim = (224,224,3)\n",
    "        self.model_score = 0\n",
    "        self.fit = 0\n",
    "        \n",
    "        # Datagen init\n",
    "        self.train_generator, self.validation_generator = datagen(self.resize_dim, self.batch_size)\n",
    "        \n",
    "        # Model init\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        # Making the predefined VGG-16 CNN architecture\n",
    "        self.model.add(ZeroPadding2D((1,1),input_shape=self.resize_dim[:2]+(3,)))\n",
    "        \n",
    "        self.model = VGG_add_conv_layer(self.model, 64, 2)\n",
    "        \n",
    "        self.model = VGG_add_conv_layer(self.model, 128, 2)\n",
    "        \n",
    "        self.model = VGG_add_conv_layer(self.model, 256, 3)\n",
    "        \n",
    "        self.model = VGG_add_conv_layer(self.model, 512, 3)\n",
    "        \n",
    "        self.model = VGG_add_conv_layer(self.model, 512, 2)\n",
    "        \n",
    "        self.model = VGG_add_conv_layer(self.model, 512, 1, trainable=True)\n",
    "        \n",
    "        self.model.add(Flatten())\n",
    "        \n",
    "        # If a path is specified, load the pre-trained weights into the architecture        \n",
    "        if weights_path:\n",
    "            self.model.load_weights(weights_path)\n",
    "        \n",
    "        # Adding the specified dense-layer architecture to the end of the VGG-16 Conv.layers.\n",
    "        for i in range(len(dense_layers)):\n",
    "            self.model.add(Dense(dense_layers[i]))\n",
    "            self.model.add(Activation('relu'))\n",
    "            self.model.add(Dropout(0.5))\n",
    "        \n",
    "        # Adding the last layer with three options (due to three classes)\n",
    "        self.model.add(Dense(3))\n",
    "        self.model.add(Activation('sigmoid'))\n",
    "    \n",
    "        # Defining the optimizer used for backprobagation\n",
    "        adam_op = optimizers.Adam(lr=self.lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "        \n",
    "        # Compiling the model\n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=adam_op,\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        print(\"Model was created succesfully\\n\")  \n",
    "    \n",
    "    def train_model(self, batch_size = 16, n_epochs=50, score_batch=16, verbose=2):\n",
    "        self.fit = self.model.fit_generator(\n",
    "                    self.train_generator,\n",
    "                    steps_per_epoch=1000 // batch_size,\n",
    "                    epochs=n_epochs,\n",
    "                    validation_data=self.validation_generator,\n",
    "                    validation_steps=400 // batch_size,\n",
    "                    verbose = verbose)\n",
    "        print(\"Training Done\")\n",
    "\n",
    "    def validate_model(self, score_batch=64):\n",
    "        self.model_score = self.model.evaluate_generator(self.validation_generator, score_batch)\n",
    "        print(\"Validation Done\")\n",
    "        return self.model_score[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling \n",
    "\n",
    "### Training parameters\n",
    "To easily compare the different models, some training parameters are setup so that the final dense layers, the learning rate, the batch-size, and the number of epochs are the same for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters (the same for both models for better comparison)\n",
    "dense_end = [32] # The dense layers added to the conv.layers (both models)\n",
    "adam_lr = 0.0005\n",
    "b_size = 32\n",
    "number_of_epochs = 100\n",
    "print_amount = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "In the following code 3 models are created and trained. To make the training flexible it is setup so that one can define how many times each type of model shall be created and trained (n_runs). Each model uses the predefined parameters defined above.\n",
    "\n",
    "To keep track of the model performances, the final training and validation score is saved in a textfile called 00_data.txt and a graph for each created model is saved in a folder, appropriately, called Graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "n_runs = 3\n",
    "n_trainingIms = 35\n",
    "n_validationIms = 30\n",
    "\n",
    "try:\n",
    "    text_file = open('00_data.txt', 'a')\n",
    "    text_file.write(\"\\n\\n\")\n",
    "    text_file.write(\"equal_set_data_(trainIms %d)(valIms %d)%s, VGG locked except last layer, epochs: %d\\n\"%(n_trainingIms, n_validationIms, str(dense_end), number_of_epochs))\n",
    "    text_file.close()\n",
    "\n",
    "    for i in range(n_runs):\n",
    "        print(\"\\nOwn_model training\")\n",
    "        baseline_model = CNN_model(conv_layers=[32,64,64], dense_layers=dense_end, learning_rate=adam_lr, batch_size=b_size)\n",
    "        print(\"Training model...\")\n",
    "        baseline_model.train_model(batch_size = b_size, n_epochs=number_of_epochs, score_batch=b_size, verbose=print_amount)\n",
    "        \n",
    "        print(\"\\nVGG16 training\")\n",
    "        VGG16_model = CNN_model_VGG16(dense_layers = dense_end, learning_rate = adam_lr, batch_size = b_size)\n",
    "        print(\"Training model...\")\n",
    "        VGG16_model.train_model(batch_size = b_size, n_epochs=number_of_epochs, score_batch=b_size, verbose=print_amount)\n",
    "        \n",
    "        print(\"\\nVGG16 untrained training\")\n",
    "        VGG16_UT_model = CNN_model_VGG16(dense_layers = dense_end, weights_path='', learning_rate = adam_lr, batch_size = b_size)\n",
    "        print(\"Training model...\")\n",
    "        VGG16_UT_model.train_model(batch_size = b_size, n_epochs=number_of_epochs, score_batch=b_size, verbose=print_amount)\n",
    "        \n",
    "        show_graph([baseline_model, VGG16_model, VGG16_UT_model], [\"Baseline\", \"VGG16 Pretrained\", \"VGG16 Untrained\"])\n",
    "        model_graph([baseline_model, VGG16_model, VGG16_UT_model], [\"Baseline\", \"VGG16 Pretrained\", \"VGG16 Untrained\"],\"run_%d_lr_%.5f_%s\"%(i,adam_lr,str(dense_end)))\n",
    "        \n",
    "        text_file = open('00_data.txt', 'a')\n",
    "        \n",
    "        text_file.write(\"Baseline: [%.4f,%.4f] \\t VGG16_model: [%.4f,%.4f] \\t VGG16_UT_model: [%.4f,%.4f] \\t %s \\n\"%\\\n",
    "                        (baseline_model.fit.history['acc'][-1], baseline_model.fit.history['val_acc'][-1], \\\n",
    "                         VGG16_model.fit.history['acc'][-1], VGG16_model.fit.history['val_acc'][-1], \\\n",
    "                         VGG16_UT_model.fit.history['acc'][-1], VGG16_UT_model.fit.history['val_acc'][-1], \\\n",
    "                         str(datetime.now())))\n",
    "        \n",
    "        text_file.close()\n",
    "\n",
    "        print(\"set %d/%d done!\"%(i+1,n_runs))\n",
    "\n",
    "    print(\"run done\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"run done\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
